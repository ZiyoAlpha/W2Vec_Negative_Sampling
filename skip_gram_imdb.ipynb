{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38abf0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "data = pd.read_csv(\"./IMDB Dataset.csv\")\n",
    "texts = data.iloc[:,0].values\n",
    "dict_list = [{\"text1\": text} for text in texts]\n",
    "dataset = Dataset.from_list(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "867f1d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text1': \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(dict_list)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9db5eca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 50000/50000 [00:04<00:00, 12106.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def f(data):\n",
    "    text = []\n",
    "    for i in data['text1']:\n",
    "        i = i.lower().replace('<br />',' ')\n",
    "        # 去除标点符号\n",
    "        i = re.sub('\\W', ' ', i) #\\w 是非字母数字下划线\n",
    "        # 去除连续空格\n",
    "\n",
    "        i = re.sub('\\s{2,}' , ' ',i)\n",
    "        # 去除数字，避免记住集体数值\n",
    "        i = re.sub( '\\d+','<num>',i)\n",
    "        #切词\n",
    "        i = i.strip().split(' ')\n",
    "        text.append(i)\n",
    "\n",
    "    return{\"text2\" :text}\n",
    "\n",
    "\n",
    "dataset = dataset.map(f,\n",
    "                      batched=True,\n",
    "                      batch_size=1000,\n",
    "                      num_proc=4,\n",
    "                      remove_columns=['text1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "750af026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=4): 100%|██████████| 50000/50000 [00:04<00:00, 12033.53 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49963,\n",
       " {'text2': ['one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'other',\n",
       "   'reviewers',\n",
       "   'has',\n",
       "   'mentioned',\n",
       "   'that',\n",
       "   'after',\n",
       "   'watching',\n",
       "   'just',\n",
       "   '<num>',\n",
       "   'oz',\n",
       "   'episode',\n",
       "   'you',\n",
       "   'll',\n",
       "   'be',\n",
       "   'hooked',\n",
       "   'they',\n",
       "   'are',\n",
       "   'right',\n",
       "   'as',\n",
       "   'this',\n",
       "   'is',\n",
       "   'exactly',\n",
       "   'what',\n",
       "   'happened',\n",
       "   'with',\n",
       "   'me',\n",
       "   'the',\n",
       "   'first',\n",
       "   'thing',\n",
       "   'that',\n",
       "   'struck',\n",
       "   'me',\n",
       "   'about',\n",
       "   'oz',\n",
       "   'was',\n",
       "   'its',\n",
       "   'brutality',\n",
       "   'and',\n",
       "   'unflinching',\n",
       "   'scenes',\n",
       "   'of',\n",
       "   'violence',\n",
       "   'which',\n",
       "   'set',\n",
       "   'in',\n",
       "   'right',\n",
       "   'from',\n",
       "   'the',\n",
       "   'word',\n",
       "   'go',\n",
       "   'trust',\n",
       "   'me',\n",
       "   'this',\n",
       "   'is',\n",
       "   'not',\n",
       "   'a',\n",
       "   'show',\n",
       "   'for',\n",
       "   'the',\n",
       "   'faint',\n",
       "   'hearted',\n",
       "   'or',\n",
       "   'timid',\n",
       "   'this',\n",
       "   'show',\n",
       "   'pulls',\n",
       "   'no',\n",
       "   'punches',\n",
       "   'with',\n",
       "   'regards',\n",
       "   'to',\n",
       "   'drugs',\n",
       "   'sex',\n",
       "   'or',\n",
       "   'violence',\n",
       "   'its',\n",
       "   'is',\n",
       "   'hardcore',\n",
       "   'in',\n",
       "   'the',\n",
       "   'classic',\n",
       "   'use',\n",
       "   'of',\n",
       "   'the',\n",
       "   'word',\n",
       "   'it',\n",
       "   'is',\n",
       "   'called',\n",
       "   'oz',\n",
       "   'as',\n",
       "   'that',\n",
       "   'is',\n",
       "   'the',\n",
       "   'nickname',\n",
       "   'given',\n",
       "   'to',\n",
       "   'the',\n",
       "   'oswald',\n",
       "   'maximum',\n",
       "   'security',\n",
       "   'state',\n",
       "   'penitentary',\n",
       "   'it',\n",
       "   'focuses',\n",
       "   'mainly',\n",
       "   'on',\n",
       "   'emerald',\n",
       "   'city',\n",
       "   'an',\n",
       "   'experimental',\n",
       "   'section',\n",
       "   'of',\n",
       "   'the',\n",
       "   'prison',\n",
       "   'where',\n",
       "   'all',\n",
       "   'the',\n",
       "   'cells',\n",
       "   'have',\n",
       "   'glass',\n",
       "   'fronts',\n",
       "   'and',\n",
       "   'face',\n",
       "   'inwards',\n",
       "   'so',\n",
       "   'privacy',\n",
       "   'is',\n",
       "   'not',\n",
       "   'high',\n",
       "   'on',\n",
       "   'the',\n",
       "   'agenda',\n",
       "   'em',\n",
       "   'city',\n",
       "   'is',\n",
       "   'home',\n",
       "   'to',\n",
       "   'many',\n",
       "   'aryans',\n",
       "   'muslims',\n",
       "   'gangstas',\n",
       "   'latinos',\n",
       "   'christians',\n",
       "   'italians',\n",
       "   'irish',\n",
       "   'and',\n",
       "   'more',\n",
       "   'so',\n",
       "   'scuffles',\n",
       "   'death',\n",
       "   'stares',\n",
       "   'dodgy',\n",
       "   'dealings',\n",
       "   'and',\n",
       "   'shady',\n",
       "   'agreements',\n",
       "   'are',\n",
       "   'never',\n",
       "   'far',\n",
       "   'away',\n",
       "   'i',\n",
       "   'would',\n",
       "   'say',\n",
       "   'the',\n",
       "   'main',\n",
       "   'appeal',\n",
       "   'of',\n",
       "   'the',\n",
       "   'show',\n",
       "   'is',\n",
       "   'due',\n",
       "   'to',\n",
       "   'the',\n",
       "   'fact',\n",
       "   'that',\n",
       "   'it',\n",
       "   'goes',\n",
       "   'where',\n",
       "   'other',\n",
       "   'shows',\n",
       "   'wouldn',\n",
       "   't',\n",
       "   'dare',\n",
       "   'forget',\n",
       "   'pretty',\n",
       "   'pictures',\n",
       "   'painted',\n",
       "   'for',\n",
       "   'mainstream',\n",
       "   'audiences',\n",
       "   'forget',\n",
       "   'charm',\n",
       "   'forget',\n",
       "   'romance',\n",
       "   'oz',\n",
       "   'doesn',\n",
       "   't',\n",
       "   'mess',\n",
       "   'around',\n",
       "   'the',\n",
       "   'first',\n",
       "   'episode',\n",
       "   'i',\n",
       "   'ever',\n",
       "   'saw',\n",
       "   'struck',\n",
       "   'me',\n",
       "   'as',\n",
       "   'so',\n",
       "   'nasty',\n",
       "   'it',\n",
       "   'was',\n",
       "   'surreal',\n",
       "   'i',\n",
       "   'couldn',\n",
       "   't',\n",
       "   'say',\n",
       "   'i',\n",
       "   'was',\n",
       "   'ready',\n",
       "   'for',\n",
       "   'it',\n",
       "   'but',\n",
       "   'as',\n",
       "   'i',\n",
       "   'watched',\n",
       "   'more',\n",
       "   'i',\n",
       "   'developed',\n",
       "   'a',\n",
       "   'taste',\n",
       "   'for',\n",
       "   'oz',\n",
       "   'and',\n",
       "   'got',\n",
       "   'accustomed',\n",
       "   'to',\n",
       "   'the',\n",
       "   'high',\n",
       "   'levels',\n",
       "   'of',\n",
       "   'graphic',\n",
       "   'violence',\n",
       "   'not',\n",
       "   'just',\n",
       "   'violence',\n",
       "   'but',\n",
       "   'injustice',\n",
       "   'crooked',\n",
       "   'guards',\n",
       "   'who',\n",
       "   'll',\n",
       "   'be',\n",
       "   'sold',\n",
       "   'out',\n",
       "   'for',\n",
       "   'a',\n",
       "   'nickel',\n",
       "   'inmates',\n",
       "   'who',\n",
       "   'll',\n",
       "   'kill',\n",
       "   'on',\n",
       "   'order',\n",
       "   'and',\n",
       "   'get',\n",
       "   'away',\n",
       "   'with',\n",
       "   'it',\n",
       "   'well',\n",
       "   'mannered',\n",
       "   'middle',\n",
       "   'class',\n",
       "   'inmates',\n",
       "   'being',\n",
       "   'turned',\n",
       "   'into',\n",
       "   'prison',\n",
       "   'bitches',\n",
       "   'due',\n",
       "   'to',\n",
       "   'their',\n",
       "   'lack',\n",
       "   'of',\n",
       "   'street',\n",
       "   'skills',\n",
       "   'or',\n",
       "   'prison',\n",
       "   'experience',\n",
       "   'watching',\n",
       "   'oz',\n",
       "   'you',\n",
       "   'may',\n",
       "   'become',\n",
       "   'comfortable',\n",
       "   'with',\n",
       "   'what',\n",
       "   'is',\n",
       "   'uncomfortable',\n",
       "   'viewing',\n",
       "   'thats',\n",
       "   'if',\n",
       "   'you',\n",
       "   'can',\n",
       "   'get',\n",
       "   'in',\n",
       "   'touch',\n",
       "   'with',\n",
       "   'your',\n",
       "   'darker',\n",
       "   'side']})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#过滤长度\n",
    "def f_len(data):\n",
    "    return [len(set(i)) > 12 + 6 + 1 for i in data['text2']]\n",
    "\n",
    "\n",
    "dataset = dataset.filter(f_len, batched=True, batch_size=1000, num_proc=4)\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "afa5277b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'other',\n",
       " 'reviewers',\n",
       " 'has',\n",
       " 'mentioned',\n",
       " 'that',\n",
       " 'after',\n",
       " 'watching',\n",
       " 'just',\n",
       " '<num>',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'you',\n",
       " 'll',\n",
       " 'be',\n",
       " 'hooked',\n",
       " 'they',\n",
       " 'are',\n",
       " 'right',\n",
       " 'as',\n",
       " 'this',\n",
       " 'is',\n",
       " 'exactly',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'with',\n",
       " 'me',\n",
       " 'the',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'struck',\n",
       " 'me',\n",
       " 'about',\n",
       " 'oz',\n",
       " 'was',\n",
       " 'its',\n",
       " 'brutality',\n",
       " 'and',\n",
       " 'unflinching',\n",
       " 'scenes',\n",
       " 'of',\n",
       " 'violence',\n",
       " 'which',\n",
       " 'set',\n",
       " 'in',\n",
       " 'right',\n",
       " 'from',\n",
       " 'the',\n",
       " 'word',\n",
       " 'go',\n",
       " 'trust',\n",
       " 'me',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'show',\n",
       " 'for',\n",
       " 'the',\n",
       " 'faint',\n",
       " 'hearted',\n",
       " 'or',\n",
       " 'timid',\n",
       " 'this',\n",
       " 'show',\n",
       " 'pulls',\n",
       " 'no',\n",
       " 'punches',\n",
       " 'with',\n",
       " 'regards',\n",
       " 'to',\n",
       " 'drugs',\n",
       " 'sex',\n",
       " 'or',\n",
       " 'violence',\n",
       " 'its',\n",
       " 'is',\n",
       " 'hardcore',\n",
       " 'in',\n",
       " 'the',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'word',\n",
       " 'it',\n",
       " 'is',\n",
       " 'called',\n",
       " 'oz',\n",
       " 'as',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'nickname',\n",
       " 'given',\n",
       " 'to',\n",
       " 'the',\n",
       " 'oswald',\n",
       " 'maximum',\n",
       " 'security',\n",
       " 'state',\n",
       " 'penitentary',\n",
       " 'it',\n",
       " 'focuses',\n",
       " 'mainly',\n",
       " 'on',\n",
       " 'emerald',\n",
       " 'city',\n",
       " 'an',\n",
       " 'experimental',\n",
       " 'section',\n",
       " 'of',\n",
       " 'the',\n",
       " 'prison',\n",
       " 'where',\n",
       " 'all',\n",
       " 'the',\n",
       " 'cells',\n",
       " 'have',\n",
       " 'glass',\n",
       " 'fronts',\n",
       " 'and',\n",
       " 'face',\n",
       " 'inwards',\n",
       " 'so',\n",
       " 'privacy',\n",
       " 'is',\n",
       " 'not',\n",
       " 'high',\n",
       " 'on',\n",
       " 'the',\n",
       " 'agenda',\n",
       " 'em',\n",
       " 'city',\n",
       " 'is',\n",
       " 'home',\n",
       " 'to',\n",
       " 'many',\n",
       " 'aryans',\n",
       " 'muslims',\n",
       " 'gangstas',\n",
       " 'latinos',\n",
       " 'christians',\n",
       " 'italians',\n",
       " 'irish',\n",
       " 'and',\n",
       " 'more',\n",
       " 'so',\n",
       " 'scuffles',\n",
       " 'death',\n",
       " 'stares',\n",
       " 'dodgy',\n",
       " 'dealings',\n",
       " 'and',\n",
       " 'shady',\n",
       " 'agreements',\n",
       " 'are',\n",
       " 'never',\n",
       " 'far',\n",
       " 'away',\n",
       " 'i',\n",
       " 'would',\n",
       " 'say',\n",
       " 'the',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'of',\n",
       " 'the',\n",
       " 'show',\n",
       " 'is',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'it',\n",
       " 'goes',\n",
       " 'where',\n",
       " 'other',\n",
       " 'shows',\n",
       " 'wouldn',\n",
       " 't',\n",
       " 'dare',\n",
       " 'forget',\n",
       " 'pretty',\n",
       " 'pictures',\n",
       " 'painted',\n",
       " 'for',\n",
       " 'mainstream',\n",
       " 'audiences',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romance',\n",
       " 'oz',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'the',\n",
       " 'first',\n",
       " 'episode',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'me',\n",
       " 'as',\n",
       " 'so',\n",
       " 'nasty',\n",
       " 'it',\n",
       " 'was',\n",
       " 'surreal',\n",
       " 'i',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'say',\n",
       " 'i',\n",
       " 'was',\n",
       " 'ready',\n",
       " 'for',\n",
       " 'it',\n",
       " 'but',\n",
       " 'as',\n",
       " 'i',\n",
       " 'watched',\n",
       " 'more',\n",
       " 'i',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'taste',\n",
       " 'for',\n",
       " 'oz',\n",
       " 'and',\n",
       " 'got',\n",
       " 'accustomed',\n",
       " 'to',\n",
       " 'the',\n",
       " 'high',\n",
       " 'levels',\n",
       " 'of',\n",
       " 'graphic',\n",
       " 'violence',\n",
       " 'not',\n",
       " 'just',\n",
       " 'violence',\n",
       " 'but',\n",
       " 'injustice',\n",
       " 'crooked',\n",
       " 'guards',\n",
       " 'who',\n",
       " 'll',\n",
       " 'be',\n",
       " 'sold',\n",
       " 'out',\n",
       " 'for',\n",
       " 'a',\n",
       " 'nickel',\n",
       " 'inmates',\n",
       " 'who',\n",
       " 'll',\n",
       " 'kill',\n",
       " 'on',\n",
       " 'order',\n",
       " 'and',\n",
       " 'get',\n",
       " 'away',\n",
       " 'with',\n",
       " 'it',\n",
       " 'well',\n",
       " 'mannered',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'inmates',\n",
       " 'being',\n",
       " 'turned',\n",
       " 'into',\n",
       " 'prison',\n",
       " 'bitches',\n",
       " 'due',\n",
       " 'to',\n",
       " 'their',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'street',\n",
       " 'skills',\n",
       " 'or',\n",
       " 'prison',\n",
       " 'experience',\n",
       " 'watching',\n",
       " 'oz',\n",
       " 'you',\n",
       " 'may',\n",
       " 'become',\n",
       " 'comfortable',\n",
       " 'with',\n",
       " 'what',\n",
       " 'is',\n",
       " 'uncomfortable',\n",
       " 'viewing',\n",
       " 'thats',\n",
       " 'if',\n",
       " 'you',\n",
       " 'can',\n",
       " 'get',\n",
       " 'in',\n",
       " 'touch',\n",
       " 'with',\n",
       " 'your',\n",
       " 'darker',\n",
       " 'side']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['text2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7964b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 189\n",
      "10000 52104\n",
      "20000 69094\n",
      "30000 81402\n",
      "40000 91684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100507, 1206)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "    # 构建字典\n",
    "def get_vocab():\n",
    "    vocab = {}\n",
    "    for i in range(len(dataset)):\n",
    "        sent = dataset[i]['text2']\n",
    "        for word in sent:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i,len(vocab))\n",
    "    \n",
    "    return vocab\n",
    " \n",
    "vocab = get_vocab()\n",
    "\n",
    "len(vocab), vocab['girl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75c743e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 49963/49963 [00:06<00:00, 7465.87 examples/s] \n"
     ]
    }
   ],
   "source": [
    "def f(data):\n",
    "    sents = []\n",
    "    for sent in data['text2']:\n",
    "        sent = [vocab[word] for word in sent]\n",
    "        sents.append(sent)\n",
    "\n",
    "    return {\"text\":sents}\n",
    "\n",
    "dataset = dataset.map(f,\n",
    "                      batched=True,\n",
    "                      batch_size=1000,\n",
    "                      num_proc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3aac317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49963,\n",
       " [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  2,\n",
       "  29,\n",
       "  30,\n",
       "  7,\n",
       "  31,\n",
       "  28,\n",
       "  32,\n",
       "  12,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  1,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  20,\n",
       "  43,\n",
       "  2,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  28,\n",
       "  22,\n",
       "  23,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  2,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  22,\n",
       "  49,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  27,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  53,\n",
       "  39,\n",
       "  34,\n",
       "  23,\n",
       "  62,\n",
       "  42,\n",
       "  2,\n",
       "  63,\n",
       "  64,\n",
       "  1,\n",
       "  2,\n",
       "  44,\n",
       "  65,\n",
       "  23,\n",
       "  66,\n",
       "  12,\n",
       "  21,\n",
       "  7,\n",
       "  23,\n",
       "  2,\n",
       "  67,\n",
       "  68,\n",
       "  59,\n",
       "  2,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  65,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  1,\n",
       "  2,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  2,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  36,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  23,\n",
       "  47,\n",
       "  93,\n",
       "  76,\n",
       "  2,\n",
       "  94,\n",
       "  95,\n",
       "  78,\n",
       "  23,\n",
       "  96,\n",
       "  59,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  36,\n",
       "  105,\n",
       "  91,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  36,\n",
       "  111,\n",
       "  112,\n",
       "  19,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  2,\n",
       "  119,\n",
       "  120,\n",
       "  1,\n",
       "  2,\n",
       "  49,\n",
       "  23,\n",
       "  121,\n",
       "  59,\n",
       "  2,\n",
       "  122,\n",
       "  7,\n",
       "  65,\n",
       "  123,\n",
       "  83,\n",
       "  3,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  50,\n",
       "  132,\n",
       "  133,\n",
       "  128,\n",
       "  134,\n",
       "  128,\n",
       "  135,\n",
       "  12,\n",
       "  136,\n",
       "  126,\n",
       "  137,\n",
       "  138,\n",
       "  2,\n",
       "  29,\n",
       "  13,\n",
       "  116,\n",
       "  139,\n",
       "  140,\n",
       "  31,\n",
       "  28,\n",
       "  21,\n",
       "  91,\n",
       "  141,\n",
       "  65,\n",
       "  33,\n",
       "  142,\n",
       "  116,\n",
       "  143,\n",
       "  126,\n",
       "  118,\n",
       "  116,\n",
       "  33,\n",
       "  144,\n",
       "  50,\n",
       "  65,\n",
       "  145,\n",
       "  21,\n",
       "  116,\n",
       "  146,\n",
       "  105,\n",
       "  116,\n",
       "  147,\n",
       "  48,\n",
       "  148,\n",
       "  50,\n",
       "  12,\n",
       "  36,\n",
       "  149,\n",
       "  150,\n",
       "  59,\n",
       "  2,\n",
       "  93,\n",
       "  151,\n",
       "  1,\n",
       "  152,\n",
       "  39,\n",
       "  47,\n",
       "  10,\n",
       "  39,\n",
       "  145,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  15,\n",
       "  16,\n",
       "  157,\n",
       "  158,\n",
       "  50,\n",
       "  48,\n",
       "  159,\n",
       "  160,\n",
       "  156,\n",
       "  15,\n",
       "  161,\n",
       "  76,\n",
       "  162,\n",
       "  36,\n",
       "  163,\n",
       "  115,\n",
       "  27,\n",
       "  65,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  160,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  82,\n",
       "  171,\n",
       "  121,\n",
       "  59,\n",
       "  172,\n",
       "  173,\n",
       "  1,\n",
       "  174,\n",
       "  175,\n",
       "  53,\n",
       "  82,\n",
       "  176,\n",
       "  9,\n",
       "  12,\n",
       "  14,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  27,\n",
       "  25,\n",
       "  23,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  14,\n",
       "  184,\n",
       "  163,\n",
       "  42,\n",
       "  185,\n",
       "  27,\n",
       "  186,\n",
       "  187,\n",
       "  188])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#转list\n",
    "dataset = dataset['text']\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7aad4",
   "metadata": {},
   "source": [
    "##使用传统数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0d7307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985,\n",
       " [474, 50, 0, 7, 23, 10597],\n",
       " [826, 11, 59, 4224, 209, 247, 119, 1, 36, 139, 173, 11])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    text = random.choice(dataset)\n",
    "\n",
    "    i = random.randint(3, len(text) - 4)\n",
    "    x = text[i]\n",
    "\n",
    "    pos = text[i - 3:i + 4]\n",
    "    pos.pop(3)\n",
    "\n",
    "    neg = []\n",
    "    while len(neg) != 12:\n",
    "        neg = random.sample(text, 12 + 6)\n",
    "        neg = [i for i in neg if i not in pos]\n",
    "        neg = neg[:12]\n",
    "\n",
    "    return x, pos, neg\n",
    "\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee6f0ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5643,  1273,     2,    14, 15579,    22,    33,    36,  1266,    23,\n",
       "           164,  3426,   518, 29204,  1904,  6248,     2,  2356,    65,   116,\n",
       "          1545,  1322,  4745, 10667,  1434,  5321,   173,  2415,   669,    36,\n",
       "            86,  1281,  1054,    18,   481,  9117,  2093,    50,  4154,  1599,\n",
       "            23,  2365,     2,    50,     1,    10,  2157,     2,   726,     2,\n",
       "           116, 24900,     2,   324,   296,    40,   116,    36,   184,  7548,\n",
       "            43,  1751,   909,   432,    22,   514,   686,  1137, 89593,  3050,\n",
       "             1,   105,   247,    65,   156, 19397,    59,   240,  1277,  4844,\n",
       "            36,   270,    59,   126,  3076,  1671,  8648,   145,   294,   230,\n",
       "            59,   827,   355,    32,    11,  6805,  2407,     0,    59,   648,\n",
       "           989,   662,  6358,     2,    19,    22,   485,   320, 49978,    65,\n",
       "          1294,   416,  1519,     2,  5455,   116,   443, 19185,    36,    23,\n",
       "          1657,     2,  1457,     1,  2547,     2,  2173,   732]),\n",
       " tensor([[    7,   183,    14,    11,  8203,    59],\n",
       "         [   33,   611,     2,   920,   237,   244],\n",
       "         [ 5795, 33368,  2190, 17575,    38,   253],\n",
       "         [ 2330,   514,   296,  4651,  1044,   533],\n",
       "         [ 4883,   586,  8184,  1935,  8982, 51173],\n",
       "         [  320,   105, 12458,  2242,   533,  5532],\n",
       "         [  105,   247,  6444,    48,  5578,  2518],\n",
       "         [    1,   721,  2941,   714,   637,    65],\n",
       "         [10611, 19438,     2,  5654,    33,   617],\n",
       "         [26991,    36,  5322,  4298,    21,    48],\n",
       "         [   33,    48,   194,  8688,   355,   235],\n",
       "         [    1,     2,  1370,  1495,   890,   183],\n",
       "         [   83,   237,     2,    83,   237,     2],\n",
       "         [ 2361, 19506,    36, 29205,    23,  1500],\n",
       "         [ 1298,     2,  2592,    40,  5028,   327],\n",
       "         [  116,   117,   949, 14060,   483,   443],\n",
       "         [   11,   317,   964,   743,   237,  5233],\n",
       "         [  874,    48, 45969, 30551, 22107,    27],\n",
       "         [  313,   964,    76,    50,   699,   105],\n",
       "         [   21,   114,    21,   968,  1055,  3657],\n",
       "         [  367,    86,    10,    48,  1285,  3431],\n",
       "         [   40,   686,   126,    34,    29,  1319],\n",
       "         [ 2093,   288,  1512, 27000, 27001,    76],\n",
       "         [ 1393,    48,   189,    50,   892,  1466],\n",
       "         [   50,  1512,   116,    59,   221,   699],\n",
       "         [   21, 13324,    36, 13059,    21, 45758],\n",
       "         [  492,   263,  2807,     1,   563,  7200],\n",
       "         [ 3210,  2631,   629,   481,  3224, 24098],\n",
       "         [    2,    99,    18,    47,   407,  1265],\n",
       "         [  327, 37662,  6578, 10312,     7,   968],\n",
       "         [    2,  6180,   116,     0,  6707,  1107],\n",
       "         [   23,   346,    19,  3949,  4917,   226],\n",
       "         [    2,  1791,     7,   944,    27,   847],\n",
       "         [  416,   213,   862,  3534,     2, 67020],\n",
       "         [  215,     5,    59,    27,    23,  3614],\n",
       "         [   32,   440,    79,    36, 16226,   920],\n",
       "         [  460,     8,    84,   168,  1548,    59],\n",
       "         [  163,  1731,  2569,     2,   773,     1],\n",
       "         [   36,    65,   241,    28,   116,   117],\n",
       "         [   65,    23,    48,   370,     2,  2448],\n",
       "         [ 2465,  2022,    22,    48,   518,   355],\n",
       "         [18751,    23,     2,  2873,  2874,   743],\n",
       "         [   14,   241,   163,   789,     1,   168],\n",
       "         [    1,     2, 48844,   239,   443,   826],\n",
       "         [ 7555,    36,  2504,  1574,  1387, 13252],\n",
       "         [ 3348,  3327,    47,    50,     2,  2606],\n",
       "         [   53, 97270,    27,   401,    36,  1514],\n",
       "         [  221,    65,    84,  3133,   526,    59],\n",
       "         [  598,     2,   472,    16,   116,   821],\n",
       "         [   76,  5702,    42,   644,   116,   565],\n",
       "         [ 2310,    27, 79206,  4563,    36,    56],\n",
       "         [33979, 11499, 28022,  5800,     2,  3423],\n",
       "         [    2,    29,  4671,   190, 13334,    33],\n",
       "         [   36,    23, 47751, 26856,   881,    59],\n",
       "         [  144,    59,   575,   158,     1,     2],\n",
       "         [ 1385,  1044,     1,   475,   158,     1],\n",
       "         [  688,  6559, 40875,   897,    59,    16],\n",
       "         [  694,    42,  3348,  3938,    59,   178],\n",
       "         [  190,   105,  5872,    16,  3044,   740],\n",
       "         [  270,    27,    48, 10533, 12313,    65],\n",
       "         [   59,  1427,   390,   296,    18,  2305],\n",
       "         [  139,   241,  1409,  3545,   230,  3042],\n",
       "         [13352,    23,    14,   938,    65,   237],\n",
       "         [  116,   859,    47,  1860,  4997,   526],\n",
       "         [   36,  2869,   145,   129,   732,  2328],\n",
       "         [    1,   408,  3357,    59,   239,   934],\n",
       "         [ 5461, 90850,  4394,   126,     2,   418],\n",
       "         [16288,  2376,   847,    23, 10704,  9985],\n",
       "         [ 3771,     7,   294, 35911,   418,    27],\n",
       "         [ 8139,  8491,   611,    42,   743,  5004],\n",
       "         [ 3080,    42,     0,     2, 23009,   359],\n",
       "         [   48,   235,  5541,  5653,   247,    65],\n",
       "         [   16,   105,  5862,     2,   331,  7605],\n",
       "         [  355,   535,    36,    10,  9837,   483],\n",
       "         [    2, 13771,  1086,   184,  2385,    22],\n",
       "         [   36,  9192,    19,    42,     2,  2061],\n",
       "         [  366,   749,  3161,   396,    36,    65],\n",
       "         [  707,    42,   483,    32,     0,  1797],\n",
       "         [    2,  1506,    33,    59,  6066, 49560],\n",
       "         [  219,   274,    59,    42,     7,   899],\n",
       "         [  186,   443, 19305,   701,    59,  2351],\n",
       "         [13507, 45573,     5,    48,   700,  1384],\n",
       "         [  213,   911,    65,  5828,    50,   239],\n",
       "         [ 3966,  3502,  2174,    24,  2054,   164],\n",
       "         [  699,   374,   186,   452,  2173,    10],\n",
       "         [   65,    23,    32,   194,   409,    36],\n",
       "         [14079,  2247,    25,    28,   312,    32],\n",
       "         [   34,   819,  4727,    22,   355,  1261],\n",
       "         [   42, 14234,     1, 16697,    65,   911],\n",
       "         [    2,  3168,    47,   277,  2009,    50],\n",
       "         [ 1089,    65,   403,    28,     7,     2],\n",
       "         [ 1330,   441,    36,   543,    42,     0],\n",
       "         [    2,   836,     2,   237,    47,   460],\n",
       "         [  742,  1859,   681,    65,    23,    10],\n",
       "         [   48,    11,    11,    43,     2, 18602],\n",
       "         [ 4906,  3953,    48,   514,  3792,    42],\n",
       "         [   11, 59101,    53,    16,  3362,    16],\n",
       "         [   28,    25,    22,   911,    65,  1451],\n",
       "         [ 8762,     2,  1360,    16,  1548,    59],\n",
       "         [20677, 84892,  6425,   317,   740,  8496],\n",
       "         [  707,   483,   484,    23,  7754,   890],\n",
       "         [    7,  4714,  2093,  5962,    59,    48],\n",
       "         [   91,    23,     2,  2921,     1,    22],\n",
       "         [  190,  6907,   145, 10150,   374,   916],\n",
       "         [ 1843,   239,  1843,  6409,    48, 36108],\n",
       "         [    2,  4950,   145,   366,  2231,  1264],\n",
       "         [  115,   145,     2,    33,  3531,    42],\n",
       "         [ 2614,   599,    86,  2134,   170,     2],\n",
       "         [   43,  2435,   611,    42, 31661,    65],\n",
       "         [   53,   460,   632,  5532,     2,   366],\n",
       "         [  819,    30,   288,   440,   475,   116],\n",
       "         [  355,     5,     2, 11778,    36,   438],\n",
       "         [ 1004,   305,     2,    36, 19774,  2287],\n",
       "         [   32,   440,   732,   286,   540,    36],\n",
       "         [  944,    59,   563,  1148, 15504,  2639],\n",
       "         [  194,   473,   474,  1982,   761,    65],\n",
       "         [   42,    65,   237,    20,   813,    65],\n",
       "         [  662,   586,    43,   219,   598,   247],\n",
       "         [   48,   135,   370,    65,  1888,    75],\n",
       "         [16279,  4598, 16279,   473, 12884,  8076],\n",
       "         [ 1730,    42,     2,     1,    40,  5803],\n",
       "         [ 1099,    36,  5456,    11,   237,   452],\n",
       "         [ 1134,     2,   366,     1,   483,  5130],\n",
       "         [   59,     2,   173,   197,   751,    18],\n",
       "         [   14,   545,    48,    36,   968,   163],\n",
       "         [ 5085, 90993,   145,  4299,  4022,    19],\n",
       "         [18629,   346,    48, 10283,    28,    22],\n",
       "         [ 5750,  7388,    33,   700,   247,    22]]),\n",
       " tensor([[  240,  3664,   374,  ...,   117,   890,    86],\n",
       "         [  355,    59,   776,  ...,    18, 13344,    59],\n",
       "         [   16,  5342,   355,  ...,  2438,    59, 10981],\n",
       "         ...,\n",
       "         [44400,  5472,  4946,  ...,    96,   367, 21654],\n",
       "         [28621,    75,    27,  ...,   396,   107,    33],\n",
       "         [   21,  3101,   374,  ...,   309,  1545,   686]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_batch(batch_size = 128):\n",
    "    x = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    for _ in range(batch_size):\n",
    "        data = get_data()\n",
    "        x.append(data[0])\n",
    "        pos.append(data[1])\n",
    "        neg.append(data[2])\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    pos = torch.LongTensor(pos)\n",
    "    neg = torch.LongTensor(neg)\n",
    "\n",
    "    return x, pos, neg\n",
    "\n",
    "\n",
    "get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd5a8100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(80.3872)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cos_loss(x, pos, neg):\n",
    "    #x -> [8, 150]\n",
    "    #pos -> [8, 6, 150]\n",
    "    #neg -> [8, 12, 150]\n",
    "\n",
    "    #[8, 150] -> [8, 150, 1]\n",
    "    x = x.unsqueeze(dim=2)\n",
    "\n",
    "    #[8, 6, 150],[8, 150, 1] -> [8, 6]\n",
    "    loss_pos = torch.bmm(pos, x).squeeze(dim=2)\n",
    "\n",
    "    #[8, 12, 150],[8, 150, 1] -> [8, 12]\n",
    "    loss_neg = torch.bmm(neg, -x).squeeze(dim=2)\n",
    "\n",
    "    #[8, 6] -> [8]\n",
    "    loss_pos = loss_pos.sigmoid().clip(min=1e-8).log().sum(dim=1)\n",
    "\n",
    "    #[8, 12] -> [8]\n",
    "    loss_neg = loss_neg.sigmoid().clip(min=1e-8).log().sum(dim=1)\n",
    "\n",
    "    return -(loss_pos + loss_neg).mean()\n",
    "\n",
    "\n",
    "a, b, c = torch.randn(8, 150), torch.randn(8, 6, 150), torch.randn(8, 12, 150)\n",
    "get_cos_loss(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7dcaf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = torch.nn.Embedding(num_embeddings=len(vocab),\n",
    "                                        embedding_dim=150)\n",
    "        \n",
    "        self.embed.weight.data.uniform_(-0.01,0.01)\n",
    "\n",
    "    def forward(self, x, pos, neg):\n",
    "        #编码\n",
    "        #[8] -> [8, 2]\n",
    "        x = self.embed(x)\n",
    "\n",
    "        #[8, 6] -> [8, 6, 2]\n",
    "        pos = self.embed(pos)\n",
    "\n",
    "        #[8, 12] -> [8, 12, 2]\n",
    "        neg = self.embed(neg)\n",
    "\n",
    "        return get_cos_loss(x, pos, neg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d15a7e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.4771, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model(*get_batch())## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7fc232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girl ['girl', 'guided', 'ney', 'referent', 'erect']\n",
      "bus ['bus', 'groovebox', 'defensible', 'dikker', 'hotmail']\n",
      "green ['green', 'rereads', 'nude', 'knit', 'gooble']\n",
      "doctor ['doctor', 'rifled', 'revitalize', 'nerdiness', 'sugary']\n",
      "dog ['dog', 'dimbulb', 'tarazu', 'ringer', 'deez']\n",
      "queen ['queen', 'dunneare', 'gruenberg', 'tdc', 'cegid']\n",
      "italy ['italy', 'arahan', 'musics', 'abetting', 'visage']\n"
     ]
    }
   ],
   "source": [
    "def test(test_words):\n",
    "    embed = model.embed.weight.data.clone()\n",
    "    vocab_keys = list(vocab.keys())\n",
    "\n",
    "    for word in test_words:\n",
    "        x = embed[vocab[word]]\n",
    "        score = torch.nn.functional.cosine_similarity(x, embed)\n",
    "        topk = score.topk(k=5).indices\n",
    "        topk = [vocab_keys[k] for k in topk]\n",
    "        print(word, topk)\n",
    "\n",
    "\n",
    "test(['girl', 'bus', 'green', 'doctor', 'dog', 'queen', 'italy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "761a1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "def train():\n",
    "    global model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    print(device)\n",
    "    os.makedirs('models', exist_ok=True) \n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    loss_sum = 0\n",
    "    for epoch in tqdm(range(200001)):\n",
    "        batch = get_batch(batch_size=128)\n",
    "        batch = [i.to(device) for i in batch]\n",
    "\n",
    "        loss = model(*batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        if epoch % 100000 == 0:\n",
    "            print(epoch, loss_sum/10000)\n",
    "            test(['girl', 'bus', 'green', 'doctor', 'dog', 'queen', 'italy'])\n",
    "            loss_sum = 0\n",
    "        if epoch % 100000 == 0:\n",
    "                    torch.save(model.cpu(), 'models/imdb_%d.model' % epoch)\n",
    "                    model = model.to(device)\n",
    "\n",
    "    model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ac6c125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200001 [00:00<6:24:20,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.001247706413269043\n",
      "girl ['girl', 'guided', 'toooooo', 'ney', 'curb']\n",
      "bus ['bus', 'groovebox', 'defensible', 'dikker', 'hotmail']\n",
      "green ['green', 'rereads', 'nude', 'knit', 'gooble']\n",
      "doctor ['doctor', 'rifled', 'sugary', 'impalpable', 'weybridge']\n",
      "dog ['dog', 'dimbulb', 'tarazu', 'deez', 'tuareg']\n",
      "queen ['queen', 'dunneare', 'gruenberg', 'tdc', 'cegid']\n",
      "italy ['italy', 'arahan', 'musics', 'abetting', 'visage']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100005/200001 [25:55<30:26, 54.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 123.33982315912246\n",
      "girl ['girl', 'boy', 'woman', 'young', 'man']\n",
      "bus ['bus', 'whispering', 'snell', 'munched', 'gurly']\n",
      "green ['green', 'gables', 'soylent', 'gleen', 'architecture']\n",
      "doctor ['doctor', 'insinnia', 'nun', 'who', 'reporter']\n",
      "dog ['dog', 'lazaro', 'scythe', 'pizza', 'kaas']\n",
      "queen ['queen', 'snow', 'shiksa', 'masssacre', 'manon']\n",
      "italy ['italy', 'england', 'japan', 'in', 'australia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200001/200001 [51:50<00:00, 64.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 123.27152564458846\n",
      "girl ['girl', 'boy', 'young', 'woman', 'man']\n",
      "bus ['bus', 'fremont', 'duffle', 'peaknuckle', 'cout']\n",
      "green ['green', 'cloudless', 'blue', 'red', 'gables']\n",
      "doctor ['doctor', 'kildaire', 'girl', 'suess', 'jewish']\n",
      "dog ['dog', 'lazaro', 'puppy', 'horse', 'hoarder']\n",
      "queen ['queen', 'clytemnastrae', 'victoria', 'hathcocks', 'bee']\n",
      "italy ['italy', 'excursionists', 'gore_won', 'france', 'southeastern']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95287b30",
   "metadata": {},
   "source": [
    "###使用Dataset+Dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e24b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (List[List]): 一个由句子组成的列表，每个元素是一个词列表\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.samples = []\n",
    "        for text in dataset:\n",
    "            if len(text) >= 7:\n",
    "                for i in range(3, len(text) - 3):\n",
    "                    self.samples.append((text, i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)  # 可以理解为有多少个样本（句子）\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx 是 DataLoader 自动传入的索引，但我们随机选一个句子\n",
    "        text, i = self.samples[idx]  # ✅ 无需 runtime 再随机\n",
    "        x = text[i]\n",
    "        pos = text[i - 3:i + 4]\n",
    "        pos.pop(3)\n",
    "\n",
    "        neg = []\n",
    "        while len(neg) < 12:\n",
    "            candidates = random.sample(text, min(12 + 6, len(text)))\n",
    "            neg = [w for w in candidates if w not in pos]\n",
    "        neg = neg[:12]\n",
    "        \n",
    "\n",
    "        # x = torch.LongTensor([x])\n",
    "        pos = torch.LongTensor(pos)\n",
    "        neg = torch.LongTensor(neg)\n",
    "\n",
    "        return x, pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b5df08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "my_dataset = MyDataset(dataset)\n",
    "loader = DataLoader(\n",
    "    my_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,  # 因为我们内部已经随机选句子了\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    "    # collate_fn=lambda batch: batch  # 不做 stack，返回 list of tuples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03a627da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128]), torch.Size([128, 6]), torch.Size([128, 12]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    x, pos, neg = batch\n",
    "    # x.squeeze(dim=1)\n",
    "    break\n",
    "    pass\n",
    "x.shape, pos.shape, neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2d1c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.0):\n",
    "        self.patience = patience      # 容忍次数\n",
    "        self.delta = delta            # 最小提升值\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def validate():\n",
    "  \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    indices = random.sample(range(len(my_dataset)), 200)  # 随机取200个索引\n",
    "    val_dataset = Subset(my_dataset, indices=indices)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    # val_loader = DataLoader(my_dataset, batch_size=64)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, pos, neg = batch\n",
    "            x = x.to(device)\n",
    "            pos = pos.to(device)\n",
    "            neg = neg.to(device)\n",
    "            loss = model(x, pos, neg)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "504e5786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2001 [00:00<28:45,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 4.8217 | Val Loss: 12.0714\n",
      "0 0.0\n",
      "girl ['girl', 'boy', 'young', 'woman', 'man']\n",
      "bus ['bus', 'fremont', 'snouts', 'peaknuckle', 'duffle']\n",
      "green ['green', 'cloudless', 'red', 'blue', 'gables']\n",
      "doctor ['doctor', 'girl', 'kildaire', 'suess', 'loving']\n",
      "dog ['dog', 'lazaro', 'puppy', 'geurilla', 'hoarder']\n",
      "queen ['queen', 'clytemnastrae', 'victoria', 'brobdingnag', 'hathcocks']\n",
      "italy ['italy', 'excursionists', 'gore_won', 'france', 'helsinki']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1001/2001 [13:53<14:04,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Train Loss: 4151.9016 | Val Loss: 10.4414\n",
      "1000 0.0\n",
      "girl ['girl', 'crazed', 'standing', 'corpses', 'blind']\n",
      "bus ['bus', 'down', 'getting', 'way', 'adjournment']\n",
      "green ['green', 'card', 'gasoline', 'puchase', 'naaahhh']\n",
      "doctor ['doctor', 'mixed', 'breed', 'reporter', 'substandard']\n",
      "dog ['dog', 'puppy', 'fetishists', 'boardinghouse', 'chist']\n",
      "queen ['queen', 'clown', 'damned', 'moustache', 'viciousness']\n",
      "italy ['italy', 'england', 'salerno', 'stros', 'produce']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [27:49<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000 | Train Loss: 3960.6573 | Val Loss: 10.0834\n",
      "2000 0.0\n",
      "girl ['girl', 'plummets', 'standing', 'crazed', 'cute']\n",
      "bus ['bus', 'getting', 'down', 'intercontinental', 'tells']\n",
      "green ['green', 'card', 'gasoline', 'puchase', 'naaahhh']\n",
      "doctor ['doctor', 'mixed', 'breed', 'reporter', 'crumby']\n",
      "dog ['dog', 'fetishists', 'horn', 'puppy', 'eyed']\n",
      "queen ['queen', 'clown', 'moustache', 'damned', 'drag']\n",
      "italy ['italy', 'salerno', 'england', 'southern', 'palermo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def train():\n",
    "    global model\n",
    "    early_stopper = EarlyStopping(patience=100, delta=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    print(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    loss_sum = 0\n",
    "    t = 0\n",
    "    for epoch in tqdm(range(2001)):\n",
    "\n",
    "        for batch in loader:\n",
    "            x, pos, neg = batch  # unpack 一个 batch 中的三个部分\n",
    "\n",
    "            x = x.to(device)     # shape: (1,)\n",
    "            pos = pos.to(device) # shape: (1, 6)\n",
    "            neg = neg.to(device) # shape: (1, 12)\n",
    "\n",
    "            loss = model(x, pos, neg)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            # print(loss)\n",
    "            loss_sum += loss.item()\n",
    "        t+=1    \n",
    "        # print(f\"{t}次迭代\")\n",
    "        if epoch % 1000 == 0:\n",
    "            val_loss = validate()\n",
    "            \n",
    "            print(f\"Epoch {epoch} | Train Loss: {loss_sum/1000:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            early_stopper(val_loss)\n",
    "            loss_sum = 0\n",
    "        if epoch % 1000 == 0:\n",
    "            print(epoch, loss_sum/10)\n",
    "            test(['girl', 'bus', 'green', 'doctor', 'dog', 'queen', 'italy'])\n",
    "            # loss_sum = 0\n",
    "\n",
    "\n",
    "\n",
    "        if epoch % 100000 == 0:\n",
    "            # torch.save(model.cpu(), 'models_dataloder/imdb_%d.model' % epoch)\n",
    "            # model = model.to(device)\n",
    "            pass\n",
    "\n",
    "    model = model.cpu()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb6d67a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girl ['girl', 'boy', 'young', 'woman', 'man']\n",
      "bus ['bus', 'fremont', 'duffle', 'peaknuckle', 'cout']\n",
      "green ['green', 'cloudless', 'blue', 'red', 'gables']\n",
      "doctor ['doctor', 'kildaire', 'girl', 'suess', 'jewish']\n",
      "dog ['dog', 'lazaro', 'puppy', 'horse', 'hoarder']\n",
      "queen ['queen', 'clytemnastrae', 'victoria', 'hathcocks', 'bee']\n",
      "italy ['italy', 'excursionists', 'gore_won', 'france', 'southeastern']\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('models/imdb_200000.model')\n",
    "\n",
    "test(['girl', 'bus', 'green', 'doctor', 'dog', 'queen', 'italy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
